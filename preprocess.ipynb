{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b5fa7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK resources first\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# Now import NLTK components\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04383711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "Shape: (70000, 29)\n",
      "Columns: ['text', 'admiration', 'amusement', 'anger', 'annoyance']... (+ 24 more)\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;sexuality shouldnâ€™t be a grouping category I...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  admiration  amusement  \\\n",
       "0                                    That game hurt.           0          0   \n",
       "1   >sexuality shouldnâ€™t be a grouping category I...           0          0   \n",
       "2     You do right, if you don't care then fuck 'em!           0          0   \n",
       "3                                 Man I love reddit.           0          0   \n",
       "4  [NAME] was nowhere near them, he was by the Fa...           0          0   \n",
       "\n",
       "   anger  annoyance  approval  caring  confusion  curiosity  desire  ...  \\\n",
       "0      0          0         0       0          0          0       0  ...   \n",
       "1      0          0         0       0          0          0       0  ...   \n",
       "2      0          0         0       0          0          0       0  ...   \n",
       "3      0          0         0       0          0          0       0  ...   \n",
       "4      0          0         0       0          0          0       0  ...   \n",
       "\n",
       "   love  nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0     0            0         0      0            0       0        0        1   \n",
       "1     0            0         0      0            0       0        0        0   \n",
       "2     0            0         0      0            0       0        0        0   \n",
       "3     1            0         0      0            0       0        0        0   \n",
       "4     0            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral  \n",
       "0         0        0  \n",
       "1         0        0  \n",
       "2         0        1  \n",
       "3         0        0  \n",
       "4         0        1  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load clean dataset\n",
    "df = pd.read_csv('dataset_clean.csv')\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns[:5])}... (+ {df.shape[1]-5} more)\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c139cf",
   "metadata": {},
   "source": [
    "## Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a986137c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: OMG! Check this out: https://example.com @user #awesome <3 Best thing ever ðŸ”¥!!!\n",
      "Cleaned: omg check this out awesome best thing ever\n"
     ]
    }
   ],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        \"\\U00002700-\\U000027BF\"  # dingbats\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # supplemental symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # more emoji\n",
    "        \"\\U00002600-\\U000026FF\"  # misc symbols\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_emoticon(text):\n",
    "    emoticon_pattern = r'(:\\s?\\)|:\\s?D|:\\s?\\(|:\\'\\)|<3|;\\)|:-\\)|:-D|:-\\(|:P|:-P|:v)'\n",
    "    return re.sub(emoticon_pattern, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove emojis\n",
    "    text = remove_emoji(text)\n",
    "    \n",
    "    #remove emoticons\n",
    "    text = remove_emoticon(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove mentions (@username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove hashtags (keep the text, remove #)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test the function\n",
    "sample_text = \"OMG! Check this out: https://example.com @user #awesome <3 Best thing ever ðŸ”¥!!!\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Cleaned:\", clean_text(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75351a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: this is a sample sentence with some stopwords\n",
      "Without stopwords: sample sentence stopwords\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Remove stopwords from text\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Test the function\n",
    "sample = \"this is a sample sentence with some stopwords\"\n",
    "print(\"Original:\", sample)\n",
    "print(\"Without stopwords:\", remove_stopwords(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d218d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: running runs ran better best\n",
      "Lemmatized: running run ran better best\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Lemmatize words to their base form\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Test the function\n",
    "sample = \"running runs ran better best\"\n",
    "print(\"Original:\", sample)\n",
    "print(\"Lemmatized:\", lemmatize_text(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d38a308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hey @user! Check out this AMAZING article: https://example.com #ML #AI ðŸ”¥\n",
      "Preprocessed: hey check amazing article ml ai\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # Handle missing values\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Apply all preprocessing steps\n",
    "    text = clean_text(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_text(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test complete pipeline\n",
    "sample = \"Hey @user! Check out this AMAZING article: https://example.com #ML #AI ðŸ”¥\"\n",
    "print(\"Original:\", sample)\n",
    "print(\"Preprocessed:\", preprocess_text(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643d4c5a",
   "metadata": {},
   "source": [
    "## Apply Preprocessing to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5fda1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BEFORE PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "1. That game hurt....\n",
      "   Length: 15 characters\n",
      "\n",
      "2.  >sexuality shouldnâ€™t be a grouping category It makes you different from othet ppl so imo it fits the definition of \"grouping\" ...\n",
      "   Length: 127 characters\n",
      "\n",
      "3. You do right, if you don't care then fuck 'em!...\n",
      "   Length: 46 characters\n"
     ]
    }
   ],
   "source": [
    "# Show sample data before preprocessing\n",
    "print(\"=\" * 80)\n",
    "print(\"BEFORE PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "for i in range(3):\n",
    "    print(f\"\\n{i+1}. {df['text'].iloc[i][:150]}...\")\n",
    "    print(f\"   Length: {len(df['text'].iloc[i])} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5558e483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text data...\n",
      "This may take a few minutes for large datasets...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70000/70000 [00:29<00:00, 2376.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Preprocessing completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing with progress indication\n",
    "print(\"Preprocessing text data...\")\n",
    "print(\"This may take a few minutes for large datasets...\\n\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df['text_processed'] = df['text'].progress_apply(preprocess_text)\n",
    "\n",
    "print(\"\\nâœ“ Preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9315200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data after preprocessing\n",
    "print(\"=\" * 80)\n",
    "print(\"AFTER PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "for i in range(3):\n",
    "    print(f\"\\n{i+1}. Original: {df['text'].iloc[i][:100]}...\")\n",
    "    print(f\"   Processed: {df['text_processed'].iloc[i][:100]}...\")\n",
    "    print(f\"   Length: {len(df['text'].iloc[i])} â†’ {len(df['text_processed'].iloc[i])} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98cbf75",
   "metadata": {},
   "source": [
    "## Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66391877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for empty texts after preprocessing\n",
    "empty_texts = df['text_processed'].str.strip() == ''\n",
    "empty_count = empty_texts.sum()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY CHECK\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Empty after preprocessing: {empty_count:,} ({empty_count/len(df)*100:.2f}%)\")\n",
    "print(f\"Valid texts: {len(df) - empty_count:,} ({(len(df)-empty_count)/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Show statistics\n",
    "print(f\"\\nText length statistics (after preprocessing):\")\n",
    "df['text_processed_length'] = df['text_processed'].str.len()\n",
    "print(f\"  Mean: {df['text_processed_length'].mean():.0f} characters\")\n",
    "print(f\"  Median: {df['text_processed_length'].median():.0f} characters\")\n",
    "print(f\"  Min: {df['text_processed_length'].min()}\")\n",
    "print(f\"  Max: {df['text_processed_length'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df0e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with empty preprocessed text (if any)\n",
    "df_final = df[df['text_processed'].str.strip() != ''].copy()\n",
    "\n",
    "print(f\"\\nRows before filtering: {len(df):,}\")\n",
    "print(f\"Rows after filtering: {len(df_final):,}\")\n",
    "print(f\"Rows removed: {len(df) - len(df_final):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268add74",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e1001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final dataset\n",
    "# Keep: text_processed + all emotion columns\n",
    "emotion_cols = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', \n",
    "                'caring', 'confusion', 'curiosity', 'desire', 'disappointment', \n",
    "                'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', \n",
    "                'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', \n",
    "                'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', \n",
    "                'neutral']\n",
    "\n",
    "# Create final dataset with only necessary columns\n",
    "df_preprocessed = df_final[['text_processed'] + emotion_cols].copy()\n",
    "\n",
    "# Rename text_processed to text for consistency\n",
    "df_preprocessed.rename(columns={'text_processed': 'text'}, inplace=True)\n",
    "\n",
    "print(\"Final dataset structure:\")\n",
    "print(f\"Shape: {df_preprocessed.shape}\")\n",
    "print(f\"Columns: {list(df_preprocessed.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d596133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "df_preprocessed.to_csv('dataset_preprocessed.csv', index=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"âœ“ PREPROCESSING COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Preprocessed dataset saved to: dataset_preprocessed.csv\")\n",
    "print(f\"Total samples: {len(df_preprocessed):,}\")\n",
    "print(f\"Features: {df_preprocessed.shape[1]} (1 text + 28 emotion labels)\")\n",
    "print(\"\\nDataset is ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b0794a",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_avg_length = df_final['text'].str.len().mean()\n",
    "processed_avg_length = df_preprocessed['text'].str.len().mean()\n",
    "reduction = ((original_avg_length - processed_avg_length) / original_avg_length) * 100\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPROCESSING IMPACT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Original average text length: {original_avg_length:.0f} characters\")\n",
    "print(f\"Processed average text length: {processed_avg_length:.0f} characters\")\n",
    "print(f\"Reduction: {reduction:.1f}%\")\n",
    "\n",
    "print(f\"\\nEmotion label distribution:\")\n",
    "emotion_totals = df_preprocessed[emotion_cols].sum()\n",
    "print(f\"Total emotion labels: {emotion_totals.sum():,}\")\n",
    "print(f\"Average labels per comment: {emotion_totals.sum() / len(df_preprocessed):.2f}\")\n",
    "\n",
    "print(f\"\\nTop 5 most common emotions:\")\n",
    "for i, (emotion, count) in enumerate(emotion_totals.sort_values(ascending=False).head(5).items(), 1):\n",
    "    print(f\"{i}. {emotion:15s}: {count:6,} ({count/len(df_preprocessed)*100:5.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
