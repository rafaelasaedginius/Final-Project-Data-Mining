{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10956cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5070 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*UnsupportedFieldAttributeWarning.*\")\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, hamming_loss, precision_score, recall_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c057b",
   "metadata": {},
   "source": [
    "## Load & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8f96fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 54497\n",
      "Num labels: 28\n",
      "Average labels per sample: 1.2443254\n"
     ]
    }
   ],
   "source": [
    "data_path = \"dataset_preprocessed4.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "if \"text_clean\" in df.columns:\n",
    "    text_col = \"text_clean\"\n",
    "elif \"text\" in df.columns:\n",
    "    text_col = \"text\"\n",
    "else:\n",
    "    raise ValueError(\"Tidak ditemukan kolom 'text_clean' maupun 'text' di dataset.\")\n",
    "\n",
    "weight_col = \"sample_weight\" if \"sample_weight\" in df.columns else None\n",
    "\n",
    "if weight_col is not None:\n",
    "    weights_all = df[weight_col].values.astype(np.float32)\n",
    "else:\n",
    "    weights_all = np.ones(len(df), dtype=np.float32)\n",
    "\n",
    "label_cols = [c for c in df.columns if c not in [text_col, \"text\", weight_col]]\n",
    "\n",
    "texts_all = df[text_col].astype(str).values\n",
    "labels_all = df[label_cols].values.astype(np.float32)\n",
    "\n",
    "print(\"Total samples:\", len(texts_all))\n",
    "print(\"Num labels:\", len(label_cols))\n",
    "print(\"Average labels per sample:\", labels_all.sum(axis=1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f216d25b",
   "metadata": {},
   "source": [
    "## Split Data (Train/Val/Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94dec158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 44142\n",
      "Val size: 4905\n",
      "Test size: 5450\n"
     ]
    }
   ],
   "source": [
    "X_train_val, X_test, y_train_val, y_test, w_train_val, w_test = train_test_split(\n",
    "    texts_all,\n",
    "    labels_all,\n",
    "    weights_all,\n",
    "    test_size=0.1,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val, w_train, w_val = train_test_split(\n",
    "    X_train_val,\n",
    "    y_train_val,\n",
    "    w_train_val,\n",
    "    test_size=0.1,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Val size:\", len(X_val))\n",
    "print(\"Test size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b7ee1",
   "metadata": {},
   "source": [
    "## Model Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30cc6c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "max_len = 160\n",
    "batch_size = 64  \n",
    "epochs = 5  \n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "warmup_ratio = 0.1\n",
    "label_smoothing = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b8e13e",
   "metadata": {},
   "source": [
    "## Dataset Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "008f7eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "        labels = torch.tensor(self.labels[index], dtype=torch.float)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde9c32",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be7e2d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmotionDataset(X_train, y_train, tokenizer, max_len)\n",
    "val_dataset = EmotionDataset(X_val, y_val, tokenizer, max_len)\n",
    "test_dataset = EmotionDataset(X_test, y_test, tokenizer, max_len)\n",
    "\n",
    "train_weights_tensor = torch.tensor(w_train, dtype=torch.float32)\n",
    "train_sampler = WeightedRandomSampler(weights=train_weights_tensor, num_samples=len(train_weights_tensor), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a34a69",
   "metadata": {},
   "source": [
    "## RMSNorm Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75c9cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(2).mean(-1, keepdim=True)\n",
    "        x = x * torch.rsqrt(norm + self.eps)\n",
    "        return self.weight * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b007618",
   "metadata": {},
   "source": [
    "## Custom BERT Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae59596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertMultiLabel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, dropout=0.4, n_dropout=3, num_attn_heads=4):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.num_layers = self.bert.config.num_hidden_layers\n",
    "        self.layer_indices = [self.num_layers, max(1, self.num_layers // 2), 1]\n",
    "        self.layer_weights = nn.Parameter(torch.ones(len(self.layer_indices)))\n",
    "        self.num_attn_heads = num_attn_heads\n",
    "        self.attn_heads = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, hidden_size),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(hidden_size, 1)\n",
    "                )\n",
    "                for _ in range(num_attn_heads)\n",
    "            ]\n",
    "        )\n",
    "        feat_dim = hidden_size * (3 + num_attn_heads)\n",
    "        self.rms = RMSNorm(feat_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        ff_dim = hidden_size * 2\n",
    "        self.ff_gate = nn.Linear(feat_dim, ff_dim)\n",
    "        self.ff_value = nn.Linear(feat_dim, ff_dim)\n",
    "        self.ff2 = nn.Linear(ff_dim, ff_dim)\n",
    "        self.out = nn.Linear(ff_dim, num_labels)\n",
    "        self.n_dropout = n_dropout\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        selected = [hidden_states[i] for i in self.layer_indices]\n",
    "        stack = torch.stack(selected, dim=0)\n",
    "        w = torch.softmax(self.layer_weights, dim=0).view(-1, 1, 1, 1)\n",
    "        fused = (w * stack).sum(dim=0)\n",
    "        cls_token = fused[:, 0]\n",
    "        mask = attention_mask.unsqueeze(-1).type_as(fused)\n",
    "        sum_embeddings = (fused * mask).sum(dim=1)\n",
    "        sum_mask = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        mean_pool = sum_embeddings / sum_mask\n",
    "        masked_hidden = fused.masked_fill(mask == 0, -1e9)\n",
    "        max_pool, _ = masked_hidden.max(dim=1)\n",
    "        attn_pools = []\n",
    "        for head in self.attn_heads:\n",
    "            scores = head(fused).squeeze(-1)\n",
    "            scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
    "            weights = torch.softmax(scores, dim=1)\n",
    "            pooled = torch.bmm(weights.unsqueeze(1), fused).squeeze(1)\n",
    "            attn_pools.append(pooled)\n",
    "        attn_cat = torch.cat(attn_pools, dim=1)\n",
    "        features = torch.cat([cls_token, mean_pool, max_pool, attn_cat], dim=1)\n",
    "        features = self.rms(features)\n",
    "        logits_list = []\n",
    "        for _ in range(self.n_dropout):\n",
    "            x = self.dropout(features)\n",
    "            gate = torch.sigmoid(self.ff_gate(x))\n",
    "            value = self.ff_value(x)\n",
    "            h = gate * value\n",
    "            h = self.dropout(h)\n",
    "            h = F.silu(self.ff2(h))\n",
    "            h = self.dropout(h)\n",
    "            logits_list.append(self.out(h))\n",
    "        stacked_logits = torch.stack(logits_list, dim=0)\n",
    "        logits = stacked_logits.mean(dim=0)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863fb716",
   "metadata": {},
   "source": [
    "## Initialize Model, Loss, Optimizer & Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a584f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(label_cols)\n",
    "model = CustomBertMultiLabel(model_name, num_labels)\n",
    "model.to(device)\n",
    "\n",
    "y_train_sum = y_train.sum(axis=0)\n",
    "num_train_samples = y_train.shape[0]\n",
    "neg_counts = num_train_samples - y_train_sum\n",
    "pos_weight = neg_counts / (y_train_sum + 1e-5)\n",
    "pos_weight_tensor = torch.tensor(pos_weight, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "total_steps = len(train_loader) * epochs\n",
    "warmup_steps = int(warmup_ratio * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7556e1",
   "metadata": {},
   "source": [
    "## Training & Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de6deaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_labels(labels, eps):\n",
    "    return labels * (1.0 - eps) + 0.5 * eps\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device, epoch, epochs, label_smoothing):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(dataloader, total=len(dataloader), desc=f\"Train {epoch}/{epochs}\", leave=False):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        labels_smooth = smooth_labels(labels, label_smoothing)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels_smooth)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def eval_epoch(model, dataloader, criterion, device, epoch, epochs, phase, label_smoothing):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, total=len(dataloader), desc=f\"{phase} {epoch}/{epochs}\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            labels_smooth = smooth_labels(labels, label_smoothing)\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels_smooth)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            all_probs.append(probs.detach().cpu().numpy())\n",
    "            all_labels.append(labels.detach().cpu().numpy())\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    return avg_loss, all_probs, all_labels\n",
    "\n",
    "def find_best_thresholds(y_true, y_probs, grid):\n",
    "    num_labels = y_true.shape[1]\n",
    "    best_thr = np.zeros(num_labels, dtype=np.float32)\n",
    "    for j in range(num_labels):\n",
    "        y_t = y_true[:, j]\n",
    "        p_j = y_probs[:, j]\n",
    "        best_f1 = 0.0\n",
    "        best_t = 0.5\n",
    "        for thr in grid:\n",
    "            pred = (p_j >= thr).astype(int)\n",
    "            f1 = f1_score(y_t, pred, zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_t = thr\n",
    "        best_thr[j] = best_t\n",
    "    return best_thr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adafa215",
   "metadata": {},
   "source": [
    "## Initialize Training Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4acbf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_grid = np.linspace(0.1, 0.7, 13)\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "best_state_dict = None\n",
    "best_thresholds = None\n",
    "history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fec679",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d40bda88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before training:\n",
      "  Used: 8.99GB / 11.94GB\n"
     ]
    }
   ],
   "source": [
    "# Monitor GPU usage during training\n",
    "import subprocess\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    \"\"\"Get GPU memory usage\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,nounits,noheader'], \n",
    "                              capture_output=True, text=True)\n",
    "        used, total = map(int, result.stdout.strip().split(','))\n",
    "        return used, total\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "print(\"GPU Memory before training:\")\n",
    "used, total = get_gpu_memory_usage()\n",
    "if used is not None:\n",
    "    print(f\"  Used: {used/1024:.2f}GB / {total/1024:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d441251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1/5:   0%|          | 0/690 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss 1.9019 val_loss 1.9052 val_f1_macro 0.2063 time 828.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 2/5:   0%|          | 0/690 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss 1.8492 val_loss 1.9093 val_f1_macro 0.2141 time 950.0 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 3/5:   0%|          | 0/690 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss 1.8209 val_loss 1.9096 val_f1_macro 0.2138 time 1023.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 4/5:   0%|          | 0/690 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, criterion, device, epoch, epochs, label_smoothing)\n",
    "    val_loss, val_probs, val_true = eval_epoch(model, val_loader, criterion, device, epoch, epochs, \"Val\", label_smoothing)\n",
    "    thr_vec = find_best_thresholds(val_true, val_probs, threshold_grid)\n",
    "    val_pred = (val_probs >= thr_vec[None, :]).astype(int)\n",
    "    f1_macro = f1_score(val_true, val_pred, average=\"macro\", zero_division=0)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\n",
    "        \"Epoch\",\n",
    "        epoch,\n",
    "        \"train_loss\",\n",
    "        round(train_loss, 4),\n",
    "        \"val_loss\",\n",
    "        round(val_loss, 4),\n",
    "        \"val_f1_macro\",\n",
    "        round(f1_macro, 4),\n",
    "        \"time\",\n",
    "        round(elapsed, 1),\n",
    "        \"s\"\n",
    "    )\n",
    "    history.append(\n",
    "        {\n",
    "            \"epoch\": int(epoch),\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"val_loss\": float(val_loss),\n",
    "            \"val_f1_macro\": float(f1_macro)\n",
    "        }\n",
    "    )\n",
    "    if f1_macro > best_val_f1:\n",
    "        best_val_f1 = f1_macro\n",
    "        best_state_dict = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        best_thresholds = thr_vec.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fde34",
   "metadata": {},
   "source": [
    "## Load Best Model & Prepare for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c90f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_state_dict is not None:\n",
    "    model.load_state_dict(best_state_dict)\n",
    "\n",
    "if best_thresholds is None:\n",
    "    best_thresholds = np.full(num_labels, 0.5, dtype=np.float32)\n",
    "\n",
    "print(\"Best validation F1 macro:\", round(best_val_f1, 4))\n",
    "print(\"Best thresholds:\", best_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b30bbf",
   "metadata": {},
   "source": [
    "## Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0d5658",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_probs, test_true = eval_epoch(model, test_loader, criterion, device, 0, 0, \"Test\", label_smoothing)\n",
    "test_pred = (test_probs >= best_thresholds[None, :]).astype(int)\n",
    "\n",
    "test_f1_macro = f1_score(test_true, test_pred, average=\"macro\", zero_division=0)\n",
    "test_f1_micro = f1_score(test_true, test_pred, average=\"micro\", zero_division=0)\n",
    "subset_acc = accuracy_score(test_true, test_pred)\n",
    "label_acc = 1.0 - hamming_loss(test_true, test_pred)\n",
    "\n",
    "print(\"Test loss:\", round(test_loss, 4))\n",
    "print(\"Test F1 macro:\", round(test_f1_macro, 4))\n",
    "print(\"Test F1 micro:\", round(test_f1_micro, 4))\n",
    "print(\"Subset accuracy:\", round(subset_acc, 4))\n",
    "print(\"Label-wise accuracy:\", round(label_acc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28117091",
   "metadata": {},
   "source": [
    "## Per-Emotion Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffabce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_f1 = f1_score(test_true, test_pred, average=None, zero_division=0)\n",
    "per_prec = precision_score(test_true, test_pred, average=None, zero_division=0)\n",
    "per_rec = recall_score(test_true, test_pred, average=None, zero_division=0)\n",
    "\n",
    "rows = []\n",
    "for idx, emo in enumerate(label_cols):\n",
    "    rows.append(\n",
    "        {\n",
    "            \"emotion\": emo,\n",
    "            \"f1\": float(per_f1[idx]),\n",
    "            \"precision\": float(per_prec[idx]),\n",
    "            \"recall\": float(per_rec[idx])\n",
    "        }\n",
    "    )\n",
    "\n",
    "metrics_df = pd.DataFrame(rows).sort_values(\"f1\", ascending=False)\n",
    "print(\"Top 10 emotions by F1:\")\n",
    "print(metrics_df.head(10))\n",
    "\n",
    "report = classification_report(test_true, test_pred, target_names=label_cols, zero_division=0)\n",
    "print(\"Classification report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9592085",
   "metadata": {},
   "source": [
    "## Save Model & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"/kaggle/working\", exist_ok=True)\n",
    "\n",
    "model_output_path = \"/output/custom_bert_emotion_model.pt\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"model_name\": model_name,\n",
    "        \"num_labels\": num_labels,\n",
    "        \"max_len\": max_len,\n",
    "        \"label_cols\": label_cols,\n",
    "        \"thresholds\": best_thresholds.tolist()\n",
    "    },\n",
    "    model_output_path\n",
    ")\n",
    "\n",
    "history_path = \"/output/training_history.json\"\n",
    "with open(history_path, \"w\") as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "summary = {\n",
    "    \"best_val_f1_macro\": float(best_val_f1),\n",
    "    \"test_loss\": float(test_loss),\n",
    "    \"test_f1_macro\": float(test_f1_macro),\n",
    "    \"test_f1_micro\": float(test_f1_micro),\n",
    "    \"subset_accuracy\": float(subset_acc),\n",
    "    \"label_accuracy\": float(label_acc),\n",
    "    \"num_labels\": int(num_labels),\n",
    "    \"num_train\": int(len(X_train)),\n",
    "    \"num_val\": int(len(X_val)),\n",
    "    \"num_test\": int(len(X_test))\n",
    "}\n",
    "\n",
    "summary_path = \"/output/model_summary.json\"\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "metrics_json_path = \"/output/emotion_metrics.json\"\n",
    "metrics_df.to_json(metrics_json_path, orient=\"records\", indent=2)\n",
    "\n",
    "metrics_csv_path = \"/kaggle/working/emotion_metrics.csv\"\n",
    "metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "\n",
    "print(\"Saved model to:\", model_output_path)\n",
    "print(\"Saved training history to:\", history_path)\n",
    "print(\"Saved summary to:\", summary_path)\n",
    "print(\"Saved per-emotion metrics to:\", metrics_json_path, \"and\", metrics_csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
