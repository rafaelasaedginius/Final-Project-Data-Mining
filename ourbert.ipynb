{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10956cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5070 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, hamming_loss, precision_score, recall_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c057b",
   "metadata": {},
   "source": [
    "## Load & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f96fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 57561\n",
      "Num labels: 28\n",
      "Average labels per sample: 1.2392939\n"
     ]
    }
   ],
   "source": [
    "data_path = \"dataset_preprocessed_final.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "if \"text_clean\" in df.columns:\n",
    "    text_col = \"text_clean\"\n",
    "elif \"text\" in df.columns:\n",
    "    text_col = \"text\"\n",
    "else:\n",
    "    raise ValueError(\"Tidak ditemukan kolom text_clean maupun text\")\n",
    "\n",
    "label_cols = [c for c in df.columns if c not in [text_col]]\n",
    "\n",
    "texts_all = df[text_col].astype(str).values\n",
    "labels_all = df[label_cols].values.astype(np.float32)\n",
    "\n",
    "print(\"Total samples:\", len(texts_all))\n",
    "print(\"Num labels:\", len(label_cols))\n",
    "print(\"Average labels per sample:\", labels_all.sum(axis=1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f216d25b",
   "metadata": {},
   "source": [
    "## Split Data (Train/Val/Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94dec158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 46623\n",
      "Val size: 5181\n",
      "Test size: 5757\n"
     ]
    }
   ],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    texts_all,\n",
    "    labels_all,\n",
    "    test_size=0.1,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val,\n",
    "    y_train_val,\n",
    "    test_size=0.1,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Val size:\", len(X_val))\n",
    "print(\"Test size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b7ee1",
   "metadata": {},
   "source": [
    "## Model Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30cc6c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "max_len = 128\n",
    "batch_size = 32\n",
    "epochs = 6\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "warmup_ratio = 0.1\n",
    "label_smoothing = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b8e13e",
   "metadata": {},
   "source": [
    "## Dataset Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "008f7eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "        labels = torch.tensor(self.labels[index], dtype=torch.float)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde9c32",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be7e2d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmotionDataset(X_train, y_train, tokenizer, max_len)\n",
    "val_dataset = EmotionDataset(X_val, y_val, tokenizer, max_len)\n",
    "test_dataset = EmotionDataset(X_test, y_test, tokenizer, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a34a69",
   "metadata": {},
   "source": [
    "## RMSNorm Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75c9cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(2).mean(-1, keepdim=True)\n",
    "        x = x * torch.rsqrt(norm + self.eps)\n",
    "        return self.weight * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b007618",
   "metadata": {},
   "source": [
    "## Custom BERT Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae59596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertMultiLabel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, dropout=0.3, n_dropout=3, num_attn_heads=2):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.num_layers = self.bert.config.num_hidden_layers\n",
    "        self.layer_indices = [self.num_layers, max(1, self.num_layers // 2), 1]\n",
    "        self.layer_weights = nn.Parameter(torch.ones(len(self.layer_indices)))\n",
    "        self.num_attn_heads = num_attn_heads\n",
    "        self.attn_heads = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, hidden_size),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(hidden_size, 1)\n",
    "                )\n",
    "                for _ in range(num_attn_heads)\n",
    "            ]\n",
    "        )\n",
    "        feat_dim = hidden_size * (3 + num_attn_heads)\n",
    "        self.rms = RMSNorm(feat_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        ff_dim = hidden_size * 2\n",
    "        self.ff_gate = nn.Linear(feat_dim, ff_dim)\n",
    "        self.ff_value = nn.Linear(feat_dim, ff_dim)\n",
    "        self.ff2 = nn.Linear(ff_dim, ff_dim)\n",
    "        self.out_global = nn.Linear(ff_dim, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "        self.label_query = nn.Parameter(torch.randn(num_labels, hidden_size) / math.sqrt(hidden_size))\n",
    "        self.label_ff = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        self.logit_mix = nn.Parameter(torch.tensor(0.0))\n",
    "        self.n_dropout = n_dropout\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        selected = [hidden_states[i] for i in self.layer_indices]\n",
    "        stack = torch.stack(selected, dim=0)\n",
    "        w = torch.softmax(self.layer_weights, dim=0).view(-1, 1, 1, 1)\n",
    "        fused = (w * stack).sum(dim=0)\n",
    "        cls_token = fused[:, 0]\n",
    "        mask = attention_mask.unsqueeze(-1).type_as(fused)\n",
    "        sum_embeddings = (fused * mask).sum(dim=1)\n",
    "        sum_mask = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        mean_pool = sum_embeddings / sum_mask\n",
    "        masked_hidden = fused.masked_fill(mask == 0, -1e4)\n",
    "        max_pool, _ = masked_hidden.max(dim=1)\n",
    "        attn_pools = []\n",
    "        for head in self.attn_heads:\n",
    "            scores = head(fused).squeeze(-1)\n",
    "            scores = scores.masked_fill(attention_mask == 0, -1e4)\n",
    "            weights = torch.softmax(scores, dim=1)\n",
    "            pooled = torch.bmm(weights.unsqueeze(1), fused).squeeze(1)\n",
    "            attn_pools.append(pooled)\n",
    "        if len(attn_pools) > 0:\n",
    "            attn_cat = torch.cat(attn_pools, dim=1)\n",
    "        else:\n",
    "            attn_cat = torch.zeros_like(cls_token)\n",
    "        features = torch.cat([cls_token, mean_pool, max_pool, attn_cat], dim=1)\n",
    "        features = self.rms(features)\n",
    "        logits_global_list = []\n",
    "        for _ in range(self.n_dropout):\n",
    "            x = self.dropout(features)\n",
    "            gate = torch.sigmoid(self.ff_gate(x))\n",
    "            value = self.ff_value(x)\n",
    "            h = gate * value\n",
    "            h = self.dropout(h)\n",
    "            h = F.silu(self.ff2(h))\n",
    "            h = self.dropout(h)\n",
    "            logits_global_list.append(self.out_global(h))\n",
    "        logits_global = torch.stack(logits_global_list, dim=0).mean(dim=0)\n",
    "        B, T, H = fused.shape\n",
    "        queries = self.label_query\n",
    "        attn_logits = torch.einsum(\"bth,lh->btl\", fused, queries)\n",
    "        attn_logits = attn_logits.masked_fill(attention_mask.unsqueeze(-1) == 0, -1e4)\n",
    "        attn_weights = torch.softmax(attn_logits, dim=1)\n",
    "        label_repr = torch.einsum(\"bth,btl->blh\", fused, attn_weights)\n",
    "        label_repr_flat = label_repr.reshape(B * self.num_labels, H)\n",
    "        label_logits_flat = self.label_ff(label_repr_flat)\n",
    "        logits_label = label_logits_flat.view(B, self.num_labels).contiguous().float()\n",
    "        mix = torch.sigmoid(self.logit_mix)\n",
    "        logits = mix * logits_global + (1.0 - mix) * logits_label\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863fb716",
   "metadata": {},
   "source": [
    "## Initialize Model, Loss, Optimizer & Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a584f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26355/2268796452.py:23: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(label_cols)\n",
    "model = CustomBertMultiLabel(model_name, num_labels)\n",
    "model.to(device)\n",
    "\n",
    "y_train_sum = y_train.sum(axis=0)\n",
    "num_train_samples = y_train.shape[0]\n",
    "neg_counts = num_train_samples - y_train_sum\n",
    "pos_weight = neg_counts / (y_train_sum + 1e-5)\n",
    "pos_weight_tensor = torch.tensor(pos_weight, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "total_steps = len(train_loader) * epochs\n",
    "warmup_steps = int(warmup_ratio * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "use_amp = device.type == \"cuda\"\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7556e1",
   "metadata": {},
   "source": [
    "## Training & Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de6deaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_labels(labels, eps):\n",
    "    return labels * (1.0 - eps) + 0.5 * eps\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device, epoch, epochs, label_smoothing):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for step, batch in enumerate(\n",
    "        tqdm(dataloader, total=len(dataloader), desc=f\"Train {epoch}/{epochs}\", leave=True, ncols=100),\n",
    "        1\n",
    "    ):\n",
    "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "        labels_smooth = smooth_labels(labels, label_smoothing)\n",
    "        optimizer.zero_grad()\n",
    "        if use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(input_ids, attention_mask)\n",
    "                loss = criterion(logits, labels_smooth)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels_smooth)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def eval_epoch(model, dataloader, criterion, device, epoch, epochs, phase, label_smoothing):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(\n",
    "            tqdm(dataloader, total=len(dataloader), desc=f\"{phase} {epoch}/{epochs}\", leave=True, ncols=100),\n",
    "            1\n",
    "        ):\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "            labels_smooth = smooth_labels(labels, label_smoothing)\n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    logits = model(input_ids, attention_mask)\n",
    "                    loss = criterion(logits, labels_smooth)\n",
    "            else:\n",
    "                logits = model(input_ids, attention_mask)\n",
    "                loss = criterion(logits, labels_smooth)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            all_probs.append(probs.detach().cpu().numpy())\n",
    "            all_labels.append(labels.detach().cpu().numpy())\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    return avg_loss, all_probs, all_labels\n",
    "\n",
    "def find_best_thresholds(y_true, y_probs, grid):\n",
    "    num_labels = y_true.shape[1]\n",
    "    best_thr = np.zeros(num_labels, dtype=np.float32)\n",
    "    for j in range(num_labels):\n",
    "        y_t = y_true[:, j]\n",
    "        p_j = y_probs[:, j]\n",
    "        best_f1 = 0.0\n",
    "        best_t = 0.5\n",
    "        for thr in grid:\n",
    "            pred = (p_j >= thr).astype(int)\n",
    "            f1 = f1_score(y_t, pred, zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_t = thr\n",
    "        best_thr[j] = best_t\n",
    "    return best_thr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adafa215",
   "metadata": {},
   "source": [
    "## Initialize Training Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4acbf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_grid = np.arange(0.2, 0.81, 0.05)\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "best_state_dict = None\n",
    "best_thresholds = None\n",
    "history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fec679",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d441251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1/6:   0%|                                                           | 0/1457 [00:00<?, ?it/s]/tmp/ipykernel_26355/1389919171.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Train 1/6: 100%|████████████████████████████████████████████████| 1457/1457 [03:03<00:00,  7.92it/s]\n",
      "Val 1/6:   0%|                                                              | 0/162 [00:00<?, ?it/s]/tmp/ipykernel_26355/1389919171.py:50: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Val 1/6: 100%|████████████████████████████████████████████████████| 162/162 [00:09<00:00, 17.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss 1.7831 val_loss 1.6877 val_f1_macro 0.1806 time 193.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 2/6:   0%|                                                           | 0/1457 [00:00<?, ?it/s]/tmp/ipykernel_26355/1389919171.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Train 2/6: 100%|████████████████████████████████████████████████| 1457/1457 [04:50<00:00,  5.01it/s]\n",
      "Val 2/6:   0%|                                                              | 0/162 [00:00<?, ?it/s]/tmp/ipykernel_26355/1389919171.py:50: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Val 2/6: 100%|████████████████████████████████████████████████████| 162/162 [00:11<00:00, 14.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss 1.6578 val_loss 1.6286 val_f1_macro 0.2463 time 303.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 3/6:   0%|                                                           | 0/1457 [00:00<?, ?it/s]/tmp/ipykernel_26355/1389919171.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Train 3/6: 100%|████████████████████████████████████████████████| 1457/1457 [06:10<00:00,  3.93it/s]\n",
      "Val 3/6:   0%|                                                              | 0/162 [00:00<?, ?it/s]/tmp/ipykernel_26355/1389919171.py:50: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Val 3/6: 100%|████████████████████████████████████████████████████| 162/162 [00:14<00:00, 11.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss 1.6244 val_loss 1.608 val_f1_macro 0.2638 time 385.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 4/6:   0%|                                                           | 0/1457 [00:00<?, ?it/s]/tmp/ipykernel_26355/1389919171.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Train 4/6: 100%|████████████████████████████████████████████████| 1457/1457 [06:44<00:00,  3.60it/s]\n",
      "Val 4/6:   0%|                                                              | 0/162 [00:00<?, ?it/s]/tmp/ipykernel_26355/1389919171.py:50: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Val 4/6: 100%|████████████████████████████████████████████████████| 162/162 [00:13<00:00, 12.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss 1.6083 val_loss 1.5971 val_f1_macro 0.2762 time 419.0 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 5/6:   0%|                                                           | 0/1457 [00:00<?, ?it/s]/tmp/ipykernel_26355/1389919171.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Train 5/6: 100%|████████████████████████████████████████████████| 1457/1457 [06:21<00:00,  3.82it/s]\n",
      "Val 5/6:   0%|                                                              | 0/162 [00:00<?, ?it/s]/tmp/ipykernel_26355/1389919171.py:50: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Val 5/6: 100%|████████████████████████████████████████████████████| 162/162 [00:12<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss 1.5993 val_loss 1.5911 val_f1_macro 0.2814 time 395.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 6/6:   0%|                                                           | 0/1457 [00:00<?, ?it/s]/tmp/ipykernel_26355/1389919171.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Train 6/6: 100%|████████████████████████████████████████████████| 1457/1457 [06:22<00:00,  3.81it/s]\n",
      "Val 6/6:   0%|                                                              | 0/162 [00:00<?, ?it/s]/tmp/ipykernel_26355/1389919171.py:50: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Val 6/6: 100%|████████████████████████████████████████████████████| 162/162 [00:13<00:00, 12.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train_loss 1.595 val_loss 1.5895 val_f1_macro 0.2854 time 396.2 s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, criterion, device, epoch, epochs, label_smoothing)\n",
    "    val_loss, val_probs, val_true = eval_epoch(model, val_loader, criterion, device, epoch, epochs, \"Val\", label_smoothing)\n",
    "    thr_vec = find_best_thresholds(val_true, val_probs, threshold_grid)\n",
    "    val_pred = (val_probs >= thr_vec[None, :]).astype(int)\n",
    "    f1_macro = f1_score(val_true, val_pred, average=\"macro\", zero_division=0)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\n",
    "        \"Epoch\",\n",
    "        epoch,\n",
    "        \"train_loss\",\n",
    "        round(train_loss, 4),\n",
    "        \"val_loss\",\n",
    "        round(val_loss, 4),\n",
    "        \"val_f1_macro\",\n",
    "        round(f1_macro, 4),\n",
    "        \"time\",\n",
    "        round(elapsed, 1),\n",
    "        \"s\"\n",
    "    )\n",
    "    history.append(\n",
    "        {\n",
    "            \"epoch\": int(epoch),\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"val_loss\": float(val_loss),\n",
    "            \"val_f1_macro\": float(f1_macro)\n",
    "        }\n",
    "    )\n",
    "    if f1_macro > best_val_f1:\n",
    "        best_val_f1 = f1_macro\n",
    "        best_state_dict = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        best_thresholds = thr_vec.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fde34",
   "metadata": {},
   "source": [
    "## Load Best Model & Prepare for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33c90f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation F1 macro: 0.2854\n",
      "Best thresholds: [0.8  0.8  0.8  0.75 0.65 0.8  0.8  0.8  0.8  0.75 0.75 0.8  0.8  0.8\n",
      " 0.8  0.8  0.2  0.8  0.8  0.8  0.8  0.8  0.65 0.8  0.8  0.8  0.8  0.6 ]\n"
     ]
    }
   ],
   "source": [
    "if best_state_dict is not None:\n",
    "    model.load_state_dict(best_state_dict)\n",
    "\n",
    "if best_thresholds is None:\n",
    "    best_thresholds = np.full(len(label_cols), 0.5, dtype=np.float32)\n",
    "\n",
    "print(\"Best validation F1 macro:\", round(best_val_f1, 4))\n",
    "print(\"Best thresholds:\", best_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b30bbf",
   "metadata": {},
   "source": [
    "## Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c0d5658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test 0/0:   0%|                                                             | 0/180 [00:00<?, ?it/s]/tmp/ipykernel_26355/1389919171.py:50: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Test 0/0: 100%|███████████████████████████████████████████████████| 180/180 [00:11<00:00, 16.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.5932\n",
      "Test F1 macro: 0.2801\n",
      "Test F1 micro: 0.2076\n",
      "Subset accuracy: 0.0\n",
      "Label-wise accuracy: 0.8177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_probs, test_true = eval_epoch(model, test_loader, criterion, device, 0, 0, \"Test\", label_smoothing)\n",
    "test_pred = (test_probs >= best_thresholds[None, :]).astype(int)\n",
    "\n",
    "test_f1_macro = f1_score(test_true, test_pred, average=\"macro\", zero_division=0)\n",
    "test_f1_micro = f1_score(test_true, test_pred, average=\"micro\", zero_division=0)\n",
    "subset_acc = accuracy_score(test_true, test_pred)\n",
    "label_acc = 1.0 - hamming_loss(test_true, test_pred)\n",
    "\n",
    "print(\"Test loss:\", round(test_loss, 4))\n",
    "print(\"Test F1 macro:\", round(test_f1_macro, 4))\n",
    "print(\"Test F1 micro:\", round(test_f1_micro, 4))\n",
    "print(\"Subset accuracy:\", round(subset_acc, 4))\n",
    "print(\"Label-wise accuracy:\", round(label_acc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28117091",
   "metadata": {},
   "source": [
    "## Per-Emotion Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fffabce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 emotions by F1:\n",
      "       emotion        f1  precision    recall\n",
      "15   gratitude  0.761795   0.692469  0.846547\n",
      "18        love  0.581064   0.452525  0.811594\n",
      "1    amusement  0.522472   0.463840  0.598071\n",
      "7    curiosity  0.503525   0.390625  0.708215\n",
      "0   admiration  0.486763   0.462662  0.513514\n",
      "20    optimism  0.364486   0.325905  0.413428\n",
      "27     neutral  0.333197   0.235362  0.570225\n",
      "2        anger  0.330413   0.235714  0.552301\n",
      "6    confusion  0.310857   0.212500  0.578723\n",
      "3    annoyance  0.305113   0.246947  0.399123\n"
     ]
    }
   ],
   "source": [
    "per_f1 = f1_score(test_true, test_pred, average=None, zero_division=0)\n",
    "per_prec = precision_score(test_true, test_pred, average=None, zero_division=0)\n",
    "per_rec = recall_score(test_true, test_pred, average=None, zero_division=0)\n",
    "\n",
    "rows = []\n",
    "for idx, emo in enumerate(label_cols):\n",
    "    rows.append(\n",
    "        {\n",
    "            \"emotion\": emo,\n",
    "            \"f1\": float(per_f1[idx]),\n",
    "            \"precision\": float(per_prec[idx]),\n",
    "            \"recall\": float(per_rec[idx])\n",
    "        }\n",
    "    )\n",
    "\n",
    "metrics_df = pd.DataFrame(rows).sort_values(\"f1\", ascending=False)\n",
    "print(\"Top 10 emotions by F1:\")\n",
    "print(metrics_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9592085",
   "metadata": {},
   "source": [
    "## Save Model & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad55185c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: ./output2/custom_bert_emotion_model.pt\n",
      "Saved training history to: ./output2/training_history.json\n",
      "Saved summary to: ./output2/model_summary.json\n",
      "Saved per-emotion metrics to: ./output2/emotion_metrics.json and ./output2/emotion_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./output2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model_output_path = os.path.join(output_dir, \"custom_bert_emotion_model.pt\")\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"model_name\": model_name,\n",
    "        \"num_labels\": len(label_cols),\n",
    "        \"max_len\": max_len,\n",
    "        \"label_cols\": label_cols,\n",
    "        \"thresholds\": best_thresholds.tolist()\n",
    "    },\n",
    "    model_output_path\n",
    ")\n",
    "\n",
    "history_path = os.path.join(output_dir, \"training_history.json\")\n",
    "with open(history_path, \"w\") as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "summary = {\n",
    "    \"best_val_f1_macro\": float(best_val_f1),\n",
    "    \"test_loss\": float(test_loss),\n",
    "    \"test_f1_macro\": float(test_f1_macro),\n",
    "    \"test_f1_micro\": float(test_f1_micro),\n",
    "    \"subset_accuracy\": float(subset_acc),\n",
    "    \"label_accuracy\": float(label_acc),\n",
    "    \"num_labels\": int(len(label_cols)),\n",
    "    \"num_train\": int(len(X_train)),\n",
    "    \"num_val\": int(len(X_val)),\n",
    "    \"num_test\": int(len(X_test))\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(output_dir, \"model_summary.json\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "metrics_json_path = os.path.join(output_dir, \"emotion_metrics.json\")\n",
    "metrics_df.to_json(metrics_json_path, orient=\"records\", indent=2)\n",
    "\n",
    "metrics_csv_path = os.path.join(output_dir, \"emotion_metrics.csv\")\n",
    "metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "\n",
    "print(\"Saved model to:\", model_output_path)\n",
    "print(\"Saved training history to:\", history_path)\n",
    "print(\"Saved summary to:\", summary_path)\n",
    "print(\"Saved per-emotion metrics to:\", metrics_json_path, \"and\", metrics_csv_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
